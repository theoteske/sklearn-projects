{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bb157ab",
   "metadata": {},
   "source": [
    "# Overfitting as a Concept\n",
    "\n",
    "Overfitting means the model fits the parameters too closely with regard to the particular observations in the training dataset but does not generalize well to new data; we say that the model has a high variance. The reason for the overfitting is that our model is too complex for the given training data. Common solutions to reduce the generalization error are as follows:\n",
    " - Collect more training data\n",
    " - Introduce a penalty for complexity via regularization \n",
    " - Choose a simpler model with fewer parameters\n",
    " - Reduce the dimensionality of the data\n",
    " \n",
    "### L1 and L2 Regularization Penalties\n",
    "\n",
    "We penalize large individual weights by adding the L1 or L2 norm of our weight vector to our loss function.\n",
    "$$\n",
    "L2: \\Vert \\mathbf{w} \\Vert_2^2 = \\sum_{j=1}^m w_j^2 \\\\\n",
    "L1: \\Vert \\mathbf{w} \\Vert_1 = \\sum_{j=1}^m |w_j|\n",
    "$$\n",
    "In contrast to L2 regularization, L1 regularization usually yields sparse feature vectors, and most fea- ture weights will be zero. Sparsity can be useful in practice if we have a high-dimensional dataset with many features that are irrelevant, especially in cases where we have more irrelevant dimensions than training examples. In this sense, L1 regularization can be understood as a technique for feature selection.\n",
    "\n",
    "Since the contours of an L1 regularized system are sharp, it is more likely that the optimum—that is, the intersection between the ellipses of the loss function and the boundary of the L1 diamond—is located on the axes, which encourages sparsity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6558e52",
   "metadata": {},
   "source": [
    "# Assessing Model Performance\n",
    "\n",
    "In general, it's common to use the *holdout method* to assess the performance of a model, which involves performing a train-test split and training the model on the training set, and then estimating its generalization ability by its performance on the test set. However, we often want to perform *model selection*, wherein we find the optimal model for the dataset by tuning its hyperparameter values.\n",
    "\n",
    "To accomplish this, we need to have a pseudo-test set taken from the training set on which to evaluate the different hyperparameter combinations; we call this set of data the *validation set*. Unfortunately, the success of this approach is very dependent on how we choose the validation set. So, we often decide to perform the much more robust method of *$k$-fold cross-validation*. The method is as follows:\n",
    "1. Perform a train-test split on the dataset.\n",
    "2. Randomly partition the training set into $k$ folds.\n",
    "3. Use $k-1$ folds to train the dataset on all sets of hyperparameters.\n",
    "4. Repeat step 3 $k$ times, using each fold as the validation set exactly once.\n",
    "5. Find the optimal set of hyperparameters by calculating the average performance of each set of hyperparameters over all folds $F_i$:\n",
    "$$\n",
    "E = \\frac{1}{k}\\sum_{i=1}^k E_i.\n",
    "$$\n",
    "\n",
    "In general, we choose $k=10$ folds as this has empirically been shown to provide the best trade-off between bias and variance. However, when the dataset is small, it may make sense to choose a larger value of $k$ so that there are more training instances for the model, and similarly when the dataset is large, we can choose a smaller value of $k$ (such as $k=5$) both to reduce the computational cost and to obtain lower variance in our estimate of the model's generalization ability, as the folds are more distinct from each other.\n",
    "\n",
    "Once we find the optimal hyperparameter set, we retrain the model on the entire training set and obtain a final performance evaluation on the test set. For now, we will load and prepare the Breast Cancer Wisconsin dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f049c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#import dataset\n",
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data',\n",
    "    header=None)\n",
    "\n",
    "#create numpy arrays\n",
    "X = df.loc[:, 2:].values\n",
    "y = df.loc[:, 1]\n",
    "\n",
    "#encode class labels, M is 1 and B is 0\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "#perform 80-20 train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e79367d",
   "metadata": {},
   "source": [
    "We can improve on the above in the context of classification by using *stratified $k$-fold cross-validation*, in which the proportions of class labels are preserved across each fold. This can yield better bias and variance estimates, especially in the case of class imbalance. We can implement this via the `StratifiedKFold` iterator in sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb7b8b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Accuracy: 0.9692753623188406 +/- 0.024367371956677097\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "#make logistic regression pipeline\n",
    "pipe_lr = make_pipeline(StandardScaler(),\n",
    "                       LogisticRegression(penalty='l2', max_iter=10000))\n",
    "\n",
    "#instantiate StratifiedKFold class\n",
    "kfold = StratifiedKFold(n_splits=10).split(X_train, y_train)\n",
    "\n",
    "#calculate scores the old-fashioned way\n",
    "scores=[]\n",
    "for k, (train, test) in enumerate(kfold):\n",
    "    pipe_lr.fit(X_train[train], y_train[train])\n",
    "    score = pipe_lr.score(X_train[test], y_train[test])\n",
    "    scores.append(score)\n",
    "    \n",
    "#print mean and standard deviation of cv accuracy\n",
    "mean_acc = np.mean(scores)\n",
    "std_acc = np.std(scores)\n",
    "print(\"CV Accuracy: {mean} +/- {std}\".format(mean=mean_acc, std=std_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c42113",
   "metadata": {},
   "source": [
    "We can also implement a $k$-fold cross-validation scorer in sklearn using `cross_val_score` which allows us to produce the scores above less verbosely. If the estimator is a classifier, the `y` parameter is either binary or multiclass, and the input to `cv` is an integer or is `None`, then `StratifiedKFold` is used. In all other cases, `KFold` is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4326d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Accuracy: 0.9692753623188406 +/- 0.024367371956677097\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#compute cv accuracy using cross_val_score\n",
    "scores = cross_val_score(estimator=pipe_lr,\n",
    "                         X=X_train,\n",
    "                         y=y_train,\n",
    "                         cv=10,\n",
    "                         n_jobs=1)\n",
    "print(\"CV Accuracy: {mean} +/- {std}\".format(mean=np.mean(scores), std=np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07116b33",
   "metadata": {},
   "source": [
    "Observe that we got the exact same results using both methods. Also, note that we can control how many CPUs the cross-validation is distributed across by setting the `n_jobs` parameter equal to the number of CPUs in our machine (if there are multiple available). For example, by setting `n_jobs=-1`, we can use all available CPUs to do the computation in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcc5b83",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization Methods\n",
    "\n",
    "While certain parameters are optimized during the training process (such as the weights of a neural network), others are chosen before training begins. These latter parameters are called *hyperparameters*, and need to be tuned in order to find the best model for a given dataset. We can use validation curves to come up with an initial range of values for the hyperparameters which we want to optimize. However, to find the optimal combination of hyperparameters, we need to perform a search over some set of hyperparameter combinations.\n",
    "\n",
    "### Tuning Hyperparameters via Grid Search \n",
    "\n",
    "Grid search is a brute-force, exhaustive search over all possible combinations of hyperparameters in a set that we specify. This is extremely computationally expensive, as there are combinatorially many hyperparameter configurations to go through. We implement this in sklearn using an SVM model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "385e14f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9757004830917875\n",
      "{'svc__C': 100.0, 'svc__gamma': 0.01, 'svc__kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#make svc pipeline\n",
    "pipe_svc = make_pipeline(StandardScaler(),\n",
    "                        SVC(random_state=42))\n",
    "\n",
    "#create hyperparameter grid\n",
    "param_range = [0.0001, 0.001, 0.01, 0.1, 1., 10., 100., 1000.]\n",
    "param_grid = [{'svc__C': param_range,\n",
    "              'svc__kernel': ['linear']},\n",
    "             {'svc__C': param_range,\n",
    "             'svc__gamma': param_range,\n",
    "             'svc__kernel': ['rbf']}]\n",
    "\n",
    "#perform grid search over hyperparameter grid using 10-fold CV\n",
    "gs = GridSearchCV(estimator=pipe_svc,\n",
    "                 param_grid=param_grid,\n",
    "                 scoring='accuracy',\n",
    "                 cv=10,\n",
    "                 refit=True,\n",
    "                 n_jobs=-1)\n",
    "gs.fit(X_train, y_train)\n",
    "print(gs.best_score_)\n",
    "print(gs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54db3ac4",
   "metadata": {},
   "source": [
    "Now that we've found the optimal hyperparameters based on a $10$-fold CV, we can use these optimal hyperparameters to retrain the model on the entire training set, and then evaluate this model's performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "530f33d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.956\n"
     ]
    }
   ],
   "source": [
    "#extract optimal model and evaluate on test set\n",
    "opt_svc = gs.best_estimator_\n",
    "#opt_svc.fit(X_train, y_train)\n",
    "print(\"Test accuracy: {}\".format(opt_svc.score(X_test, y_test).round(3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9679ec37",
   "metadata": {},
   "source": [
    "### Tuning Hyperparameters More Widely with Randomized Search\n",
    "\n",
    "In randomized search, we draw hyperparameter combinations randomly from a distribution over the hyperparameter space or from a discrete set of hyperparameter options. While this is not an exhaustive search over the hyperparameter space, we get to explore a wider range of hyperparameter combinations in a more computationally effective manner.\n",
    "\n",
    "While we could use the same discrete set of hyperparameters `param_range` that we defined for grid search above, the true power of the `RandomizedSearchCV` class in sklearn is that we can instead randomize over a probability distribution. So, we can use a loguniform distribution rather than the regular uniform distribution to ensure that, in the limit, the same number of samples will be drawn from the $[0.0001, 0.001]$ range as from the $[10, 100]$ range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01bdbd3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.18582273e-02, 4.51856095e+02, 1.33032451e+01, 1.55099140e+00,\n",
       "       1.23631883e-03, 1.23583828e-03, 2.55026485e-04, 1.15673272e+02,\n",
       "       1.61363417e+00, 9.04707196e+00])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.stats\n",
    "\n",
    "#create parameter range with loguniform distribution\n",
    "param_range = scipy.stats.loguniform(0.0001, 1000.0)\n",
    "\n",
    "#check behavior of the distribution\n",
    "np.random.seed(42)\n",
    "param_range.rvs(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927b041d",
   "metadata": {},
   "source": [
    "Now we can apply the `RandomizedSearchCV` class to the SVC we created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "997658bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.976\n",
      "{'svc__C': 2.3468121983173633, 'svc__gamma': 0.011733722171268566, 'svc__kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "#create parameter grid\n",
    "param_grid = [{'svc__C': param_range,\n",
    "              'svc__kernel': ['linear']},\n",
    "             {'svc__C': param_range,\n",
    "             'svc__gamma': param_range,\n",
    "             'svc__kernel': ['rbf']}]\n",
    "\n",
    "#perform randomized search over the param_grid distribution\n",
    "rs = RandomizedSearchCV(estimator=pipe_svc,\n",
    "                        param_distributions=param_grid,\n",
    "                        scoring='accuracy',\n",
    "                        refit=True,\n",
    "                        n_iter=100,\n",
    "                        cv=10,\n",
    "                        random_state=42,\n",
    "                        n_jobs=-1)\n",
    "rs = rs.fit(X_train, y_train)\n",
    "print(rs.best_score_.round(3))\n",
    "print(rs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c1fb43",
   "metadata": {},
   "source": [
    "Finally, let's see if the randomized search CV results in a better performance on the test set (indeed, it does)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e169f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.982\n"
     ]
    }
   ],
   "source": [
    "#extract optimal model and evaluate on test set\n",
    "opt_svc = rs.best_estimator_\n",
    "print('Test Accuracy: {}'.format(opt_svc.score(X_test, y_test).round(3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2bdd2b",
   "metadata": {},
   "source": [
    "### Resource-Efficient Hyperparameter Search with Successive Halving\n",
    "\n",
    "Given a large set of potential hyperparameter configurations, successive halving successively discards unpromising configurations until only one remains. The procedure can be summarized as follows:\n",
    "1. Draw a large set of candidate configurations via random sampling.\n",
    "2. Train the models with a small subset of the training set.\n",
    "3. Discard the bottom $50\\%$ of configurations based on predictive performance.\n",
    "4. Repeat steps 2 and 3 using a larger subset of the training set to train the models each time, until only one configuration is left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2c0c06e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.968\n",
      "{'svc__C': 0.06295450333384245, 'svc__kernel': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingRandomSearchCV\n",
    "\n",
    "#perform successive halving\n",
    "hs = HalvingRandomSearchCV(pipe_svc,\n",
    "                          param_distributions=param_grid,\n",
    "                          n_candidates='exhaust',\n",
    "                          resource='n_samples',\n",
    "                          factor=1.5,\n",
    "                          random_state=42,\n",
    "                          n_jobs=-1)\n",
    "hs = hs.fit(X_train, y_train)\n",
    "print(hs.best_score_.round(3))\n",
    "print(hs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f845bc02",
   "metadata": {},
   "source": [
    "We set `factor=1.5` to keep $66\\%$ of the remaining candidates each round (if we had set `factor=2`, we would keep $50\\%$ of candidates). Now we again check the performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c9631ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.982\n"
     ]
    }
   ],
   "source": [
    "#extract best model and evaluate on test set\n",
    "opt_hs = hs.best_estimator_\n",
    "print(\"Test accuracy: {}\".format(opt_hs.score(X_test, y_test).round(3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ebff6b",
   "metadata": {},
   "source": [
    "We observe that successive halving also performs very well, and similarly to randomized search CV."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff4b93a",
   "metadata": {},
   "source": [
    "### Nested Cross-Validation\n",
    "\n",
    "The idea behind nested cross-validation is to perform model selection before we then optimize the hyperparameters of the best type of model. To do this, we partition the training set into $k_1$ folds, as we did before, but now we perform a $k_2$-fold CV within each mini-training set, and evaluate each one on the corresponding test fold. This gives us an (almost) unbiased estimate of a given model's ability to generalize to the test set. After we repeat this procedure for each potential model, we choose the model with the best estimated CV accuracy. We then perform a normal grid or randomized search CV to optimize the hyperparameters of this optimal model.\n",
    "\n",
    "In sklearn, we can perform nested cross-validation with grid search on the SVC model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "33cb7165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV accuracy for SVC: 0.9602415458937198 +/- 0.03097673292462225\n"
     ]
    }
   ],
   "source": [
    "k1 = 10\n",
    "k2 = 3\n",
    "\n",
    "#create parameter grid\n",
    "param_range = [0.0001, 0.001, 0.01, 0.1, 1., 10., 100., 1000.]\n",
    "param_grid = [{'svc__C': param_range,\n",
    "              'svc__kernel': ['linear']},\n",
    "             {'svc__C': param_range,\n",
    "             'svc__gamma': param_range,\n",
    "             'svc__kernel': ['rbf']}]\n",
    "\n",
    "#create grid search object which we will apply on each mini-training set\n",
    "gs = GridSearchCV(estimator=pipe_svc,\n",
    "                 param_grid=param_grid,\n",
    "                 scoring='accuracy',\n",
    "                 cv=k2)\n",
    "\n",
    "#now perform outer loop, we iterate over all mini-training sets and apply gs object\n",
    "scores = cross_val_score(gs, X_train, y_train, \n",
    "                        scoring='accuracy', cv=10)\n",
    "print(\"CV accuracy for SVC: {mean} +/- {std}\".format(mean=np.mean(scores), std=np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8660d08b",
   "metadata": {},
   "source": [
    "The returned average cross-validation accuracy gives us a good estimate of what to expect if we tune the hyperparameters of a model and use it on unseen data. Now, we can perform the exact same procedure on a different classifier. We will use a decision tree for this example, and we will only tune its depth parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "86039a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV accuracy for SVC: 0.9183574879227054 +/- 0.044612683047718524\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#create grid search object which we will apply on each mini-training set\n",
    "gs = GridSearchCV(estimator=DecisionTreeClassifier(random_state=42),\n",
    "                 param_grid={'max_depth': [1, 2, 3, 4, 5, 6, 7, None]},\n",
    "                 scoring='accuracy',\n",
    "                 cv=k2)\n",
    "\n",
    "#now perform outer loop, we iterate over all mini-training sets and apply gs object\n",
    "scores = cross_val_score(gs, X_train, y_train,\n",
    "                        scoring='accuracy', cv=10)\n",
    "print(\"CV accuracy for SVC: {mean} +/- {std}\".format(mean=np.mean(scores), std=np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f071b2",
   "metadata": {},
   "source": [
    "From the above, we see that we can expect the SVC to perform better on unseen data than the decision tree. Thus, we should choose the SVC to be the model whose hyperparameters we optimize for, which we can do using any of the methods already described above."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
